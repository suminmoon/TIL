{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow\n",
    "### google이 만든 머신러닝을 위한 library( python, c )\n",
    "### tensorflow를 이용해보자!\n",
    "### Hello world 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello world'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tensorflow의 구성요소 (3가지)\n",
    "# 1.node : 수학적인 연산을 담당, 데이터의 입출력\n",
    "# 2.Tensor : 다차원 array(matrix)\n",
    "# 3.edge : 한 node가 가지고 있는 tensor를 다른 node로 이동\n",
    "\n",
    "my_node = tf.constant(\"Hello world\")   #node를 만들고 해당 node안에 문자열이 들어가있음\n",
    "sess = tf.Session()   #Session, runner( node를 실행시키는 놈 )\n",
    "sess.run(my_node)\n",
    "print(sess.run(my_node))  #b'Hello world'  / decode() -> 문자열 처리 하려면 붙여주기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 20.0, 30.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#          constant : 상수 node 만들 때\n",
    "node1 = tf.constant(10, dtype = tf.float32)\n",
    "node2 = tf.constant(20, dtype = tf.float32)\n",
    "\n",
    "node3 = node1 + node2   # tf.add(node1, node2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run([node1, node2, node3])\n",
    "# 상수 -> shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11., 13., 15.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# placeholder (데이터를 받아들이는 그릇)\n",
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.placeholder(dtype=tf.float32)\n",
    "node2 = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "node3 = node1 + node2\n",
    "\n",
    "sess = tf.Session()\n",
    "# sess.run(node3, feed_dict={node1:input(), node2:input()})\n",
    "# placeholder은 다른 텐서를 할당하는 것이기 때문에 할당을 위해 feed_dict가 필요 run할 때\n",
    "sess.run(node3, feed_dict={node1:[1,2,3], node2:[10,11,12]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_12:0' shape=(3,) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# node1 = tf.constant(3, dtype=tf.float32)\n",
    "# node1\n",
    "node1 = tf.constant([1,2,3], dtype=tf.float32)\n",
    "node1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [0.16898167],b: [0.47245124], cost: 25.03156089782715\n",
      "W: [1.9502418],b: [1.1131121], cost: 0.001844286802224815\n",
      "W: [1.9758297],b: [1.0549448], cost: 0.0004351726092863828\n",
      "W: [1.9882588],b: [1.0266902], cost: 0.00010268550249747932\n",
      "W: [1.9942966],b: [1.0129652], cost: 2.4231723728007637e-05\n",
      "W: [1.9972295],b: [1.0062978], cost: 5.717709427699447e-06\n",
      "W: [1.9986538],b: [1.00306], cost: 1.3498711268766783e-06\n",
      "W: [1.9993454],b: [1.0014876], cost: 3.1897698704597133e-07\n",
      "W: [1.9996814],b: [1.0007241], cost: 7.557688519455041e-08\n",
      "W: [1.9998442],b: [1.0003536], cost: 1.8032324078376405e-08\n"
     ]
    }
   ],
   "source": [
    "# 간단한 linear regression을 이용한 machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# training data set (학습 데이터 셋)\n",
    "x = [1,2,3]   # 독립변수, 입력데이터\n",
    "y = [3,5,7]   # 종속변수, 입력데이터의 label\n",
    "# W:2 b:1인데 계산 결과 이 값으로 수렴하나?\n",
    "# Weight & bias 정의\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "#                  정규분포에서 랜덤 값 추출 [뽑을 개수] (초기 값 지정)\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis( 우리가 최종적으로 알아내야 하는 직선 )\n",
    "#              데이터에 가장 인접한 직선\n",
    "#              예측모델이 만들어졌으니, prediction이 가능\n",
    "H = W * x + b \n",
    "\n",
    "# Cost function ( Loss function, 비용함수 )\n",
    "# Cost function이 최소가 되는 W와 b 값을 구하는 것이 목적\n",
    "cost = tf.reduce_mean(tf.square(H - y))\n",
    "\n",
    "# Cost function의 minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01 )\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# 학습이 진행되면서 반복되면서 W, b 값은 점점 우리가 원하는 값으로 가는 것이다.\n",
    "\n",
    "# 그래프를 실행시키기 위한 runner 역할 session\n",
    "sess = tf.Session()\n",
    "# Variable을 사용할 경우 초기화를 시켜줘야 한다.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복 실행을 통해 계속 cost 줄여가면서 Cost 값을 최소화\n",
    "for step in range(3000):\n",
    "    _, W_val, b_val, cost_val = sess.run([train, W, b, cost])\n",
    "    if  step % 300 ==0 :  #300번 반복될 때마다 출력하겠다.(안 쓰면 3000번 출력됨)\n",
    "        print(\"W: {},b: {}, cost: {}\".format(W_val, b_val, cost_val))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [2.7959564],b: [0.47559014], cost: 1.9559580087661743\n",
      "W: [2.1565785],b: [0.6440601], cost: 0.018262630328536034\n",
      "W: [2.0760589],b: [0.8270997], cost: 0.004309235140681267\n",
      "W: [2.0369463],b: [0.9160122], cost: 0.0010168147273361683\n",
      "W: [2.017947],b: [0.9592024], cost: 0.00023992701608221978\n",
      "W: [2.0087175],b: [0.98018295], cost: 5.660856913891621e-05\n",
      "W: [2.004235],b: [0.990373], cost: 1.335977140115574e-05\n",
      "W: [2.002058],b: [0.99532217], cost: 3.1543643217446515e-06\n",
      "W: [2.0010014],b: [0.99772525], cost: 7.460586743945896e-07\n",
      "W: [2.000488],b: [0.99889195], cost: 1.7701336219033692e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21.001842], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 간단한 linear regression을 이용한 machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# training data set (학습 데이터 셋)\n",
    "x_data = [1,2,3]   # 독립변수, 입력데이터\n",
    "y_data = [3,5,7]   # 종속변수, 입력데이터의 label\n",
    "# W:2 b:1인데 계산 결과 이 값으로 수렴하나?\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.placeholder(dtype=tf.float32)\n",
    "# Weight & bias 정의\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "#                  정규분포에서 랜덤 값 추출 [뽑을 개수] (초기 값 지정)\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis( 우리가 최종적으로 알아내야 하는 직선 )\n",
    "#              데이터에 가장 인접한 직선\n",
    "#              예측모델이 만들어졌으니, prediction이 가능\n",
    "H = W * x + b \n",
    "\n",
    "# Cost function ( Loss function, 비용함수 )\n",
    "# Cost function이 최소가 되는 W와 b 값을 구하는 것이 목적\n",
    "cost = tf.reduce_mean(tf.square(H - y))\n",
    "\n",
    "# Cost function의 minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01 )\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# 학습이 진행되면서 반복되면서 W, b 값은 점점 우리가 원하는 값으로 가는 것이다.\n",
    "\n",
    "# 그래프를 실행시키기 위한 runner 역할 session\n",
    "sess = tf.Session()\n",
    "# Variable을 사용할 경우 초기화를 시켜줘야 한다.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복 실행을 통해 계속 cost 줄여가면서 Cost 값을 최소화\n",
    "for step in range(3000):\n",
    "    _, W_val, b_val, cost_val = sess.run([train, W, b, cost],\n",
    "                                        feed_dict={x:x_data, y:y_data})\n",
    "    if  step % 300 ==0 :  #300번 반복될 때마다 출력하겠다.(안 쓰면 3000번 출력됨)\n",
    "        print(\"W: {},b: {}, cost: {}\".format(W_val, b_val, cost_val))\n",
    "        \n",
    "        \n",
    "#### Prediction\n",
    "sess.run(H, feed_dict={x:10})  #가설을 통해 x가 10일 때 y는 몇일까 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "### 기본적인 linear regression 예제\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# training data set\n",
    "np.random.seed(12345)\n",
    "x_data = np.arange(0, 20, 1)\n",
    "y_data = np.array([ t*2 + np.random.normal(2,2) for t in x_data ] )\n",
    "# 일단 눈으로 먼저 확인을 해보자!\n",
    "plt.scatter(x_data, y_data)\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(dtype = tf.float32)\n",
    "Y = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "# Weight & bias                   1 = 난수 몇 개 만들건지\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")  # 랜덤으로 초기 값 지정\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "H = W*X + b\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.square( H - Y ))\n",
    "\n",
    "# train\n",
    "optimizer = tf.train.GradientDescentOptimizer( learning_rate= 0.001 )\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict = {X:x_data , Y:y_data})\n",
    "    if step % 300 == 3:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "        \n",
    "# 입력데이터에 대한 처리가 이루어져야 정상적으로 학습이 진행될 수 있다. \n",
    "# 만약 학습이 정상적으로 이루어졌으면 W, b 값이 결정된다.\n",
    "\n",
    "x_line = np.arange(0, 20, 1)\n",
    "y_line = np.array([ sess.run(W) * t + sess.run(b) for t in x_line ])\n",
    "plt.plot(x_line, y_line, \"y\")\n",
    "plt.show()\n",
    "\n",
    "# Predict\n",
    "sess.run(H, feed_dict={X:15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ozone data를 이용한 linaer regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\ipykernel\\__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\ipykernel\\__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ozone</th>\n",
       "      <th>Solar.R</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>NTemp</th>\n",
       "      <th>NOzone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.239521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.209581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.065868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.101796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.131737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.107784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.041916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.089820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.059880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.077844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.101796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.077844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.197605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.173653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.059880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.059880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.017964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.185629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.131737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.263473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>115.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>79</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.682635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>37.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.215569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>29.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>82</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.167665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>71.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.419162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>87</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.227545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>23.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.131737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>21.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.119760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>37.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.215569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>85.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>94</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.502994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>96.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>91</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.568862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>78.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>92</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.461078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>73.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>93</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.431138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>91.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>93</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.538922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>47.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.275449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>32.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.185629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>20.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.113772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>23.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.131737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>21.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.119760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>24.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>73</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.137725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>44.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.257485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.119760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>28.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.161677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.047904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>13.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.071856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>46.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.269461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>18.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.101796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.071856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>24.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.137725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>16.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.089820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>13.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.071856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.131737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>36.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.209581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.035928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.077844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>30.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.173653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>14.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.077844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>18.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.101796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>20.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.113772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ozone  Solar.R  Wind  Temp  Month  Day  NTemp    NOzone\n",
       "0     41.0    190.0   7.4    67      5    1  0.250  0.239521\n",
       "1     36.0    118.0   8.0    72      5    2  0.375  0.209581\n",
       "2     12.0    149.0  12.6    74      5    3  0.425  0.065868\n",
       "3     18.0    313.0  11.5    62      5    4  0.125  0.101796\n",
       "6     23.0    299.0   8.6    65      5    7  0.200  0.131737\n",
       "7     19.0     99.0  13.8    59      5    8  0.050  0.107784\n",
       "8      8.0     19.0  20.1    61      5    9  0.100  0.041916\n",
       "11    16.0    256.0   9.7    69      5   12  0.300  0.089820\n",
       "12    11.0    290.0   9.2    66      5   13  0.225  0.059880\n",
       "13    14.0    274.0  10.9    68      5   14  0.275  0.077844\n",
       "14    18.0     65.0  13.2    58      5   15  0.025  0.101796\n",
       "15    14.0    334.0  11.5    64      5   16  0.175  0.077844\n",
       "16    34.0    307.0  12.0    66      5   17  0.225  0.197605\n",
       "17     6.0     78.0  18.4    57      5   18  0.000  0.029940\n",
       "18    30.0    322.0  11.5    68      5   19  0.275  0.173653\n",
       "19    11.0     44.0   9.7    62      5   20  0.125  0.059880\n",
       "20     1.0      8.0   9.7    59      5   21  0.050  0.000000\n",
       "21    11.0    320.0  16.6    73      5   22  0.400  0.059880\n",
       "22     4.0     25.0   9.7    61      5   23  0.100  0.017964\n",
       "23    32.0     92.0  12.0    61      5   24  0.100  0.185629\n",
       "27    23.0     13.0  12.0    67      5   28  0.250  0.131737\n",
       "28    45.0    252.0  14.9    81      5   29  0.600  0.263473\n",
       "29   115.0    223.0   5.7    79      5   30  0.550  0.682635\n",
       "30    37.0    279.0   7.4    76      5   31  0.475  0.215569\n",
       "37    29.0    127.0   9.7    82      6    7  0.625  0.167665\n",
       "39    71.0    291.0  13.8    90      6    9  0.825  0.419162\n",
       "40    39.0    323.0  11.5    87      6   10  0.750  0.227545\n",
       "43    23.0    148.0   8.0    82      6   13  0.625  0.131737\n",
       "46    21.0    191.0  14.9    77      6   16  0.500  0.119760\n",
       "47    37.0    284.0  20.7    72      6   17  0.375  0.215569\n",
       "..     ...      ...   ...   ...    ...  ...    ...       ...\n",
       "122   85.0    188.0   6.3    94      8   31  0.925  0.502994\n",
       "123   96.0    167.0   6.9    91      9    1  0.850  0.568862\n",
       "124   78.0    197.0   5.1    92      9    2  0.875  0.461078\n",
       "125   73.0    183.0   2.8    93      9    3  0.900  0.431138\n",
       "126   91.0    189.0   4.6    93      9    4  0.900  0.538922\n",
       "127   47.0     95.0   7.4    87      9    5  0.750  0.275449\n",
       "128   32.0     92.0  15.5    84      9    6  0.675  0.185629\n",
       "129   20.0    252.0  10.9    80      9    7  0.575  0.113772\n",
       "130   23.0    220.0  10.3    78      9    8  0.525  0.131737\n",
       "131   21.0    230.0  10.9    75      9    9  0.450  0.119760\n",
       "132   24.0    259.0   9.7    73      9   10  0.400  0.137725\n",
       "133   44.0    236.0  14.9    81      9   11  0.600  0.257485\n",
       "134   21.0    259.0  15.5    76      9   12  0.475  0.119760\n",
       "135   28.0    238.0   6.3    77      9   13  0.500  0.161677\n",
       "136    9.0     24.0  10.9    71      9   14  0.350  0.047904\n",
       "137   13.0    112.0  11.5    71      9   15  0.350  0.071856\n",
       "138   46.0    237.0   6.9    78      9   16  0.525  0.269461\n",
       "139   18.0    224.0  13.8    67      9   17  0.250  0.101796\n",
       "140   13.0     27.0  10.3    76      9   18  0.475  0.071856\n",
       "141   24.0    238.0  10.3    68      9   19  0.275  0.137725\n",
       "142   16.0    201.0   8.0    82      9   20  0.625  0.089820\n",
       "143   13.0    238.0  12.6    64      9   21  0.175  0.071856\n",
       "144   23.0     14.0   9.2    71      9   22  0.350  0.131737\n",
       "145   36.0    139.0  10.3    81      9   23  0.600  0.209581\n",
       "146    7.0     49.0  10.3    69      9   24  0.300  0.035928\n",
       "147   14.0     20.0  16.6    63      9   25  0.150  0.077844\n",
       "148   30.0    193.0   6.9    70      9   26  0.325  0.173653\n",
       "150   14.0    191.0  14.3    75      9   28  0.450  0.077844\n",
       "151   18.0    131.0   8.0    76      9   29  0.475  0.101796\n",
       "152   20.0    223.0  11.5    68      9   30  0.275  0.113772\n",
       "\n",
       "[111 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 0.033157460391521454, W:[0.8031056], b:[-0.07008792]\n",
      "cost : 0.024199554696679115, W:[0.77457094], b:[-0.11179072]\n",
      "cost : 0.022216347977519035, W:[0.75905377], b:[-0.12983145]\n",
      "cost : 0.021730046719312668, W:[0.7496202], b:[-0.13695502]\n",
      "cost : 0.02156744711101055, W:[0.74306023], b:[-0.13905858]\n",
      "cost : 0.021477168425917625, W:[0.73788697], b:[-0.13886984]\n",
      "cost : 0.021405216306447983, W:[0.7334111], b:[-0.13765058]\n",
      "cost : 0.021339891478419304, W:[0.72931194], b:[-0.13598391]\n",
      "cost : 0.021278539672493935, W:[0.72543967], b:[-0.13413872]\n",
      "cost : 0.02122047357261181, W:[0.721724], b:[-0.132239]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([71.683876], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## 1. Data Loading\n",
    "data = pd.read_csv(\"./data/ozone/ozone.csv\", sep=\",\")\n",
    "### - Temp와 Ozone의 관계를 알아보기 위해\n",
    "###   plt를 이용해서 그림을 그려보자!\n",
    "###   데이터를 살펴보니 NaN이 포함되어 있다.\n",
    "###   결치값을 제거하거나 다른 값으로 대체`\n",
    "###   결치값을 포함된 행을 지우고, 그림을 그려보자!\n",
    "df = data.dropna(how=\"any\", inplace = False )\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "### Normalization, Standardization\n",
    "# 이상값을 처리하기 위해!!\n",
    "### Standardization : 현재값 - 평균 / 표준편차\n",
    "### Nomalization : (현재값 - min값) / (max값 - min값) (0 ~ 1)\n",
    "df[\"NTemp\"] = (df[\"Temp\"] - df[\"Temp\"].min()) / (df[\"Temp\"].max()-df[\"Temp\"].min())      \n",
    "df[\"NOzone\"] = (df[\"Ozone\"]- df[\"Ozone\"].min()) / (df[\"Ozone\"].max()-df[\"Ozone\"].min())\n",
    "x_data = df[\"NTemp\"]\n",
    "y_data = df[\"NOzone\"]\n",
    "plt.scatter(x_data, y_data, marker=\"P\")\n",
    "X = tf.placeholder(dtype=tf.float32)\n",
    "Y = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "H = W * X + b\n",
    "\n",
    "cost = tf.reduce_mean( tf.square(H - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3000):\n",
    "    _, cost_val, w_val, b_val = sess.run([train, cost, W, b], feed_dict={X:x_data, Y:y_data} )\n",
    "    if step % 300 ==0:\n",
    "        print(\"cost : {}, W:{}, b:{}\".format(cost_val, w_val, b_val))\n",
    "        \n",
    "        \n",
    "        \n",
    "x_line = x_data\n",
    "y_line = [sess.run(W) * t + sess.run(b) for t in x_line]\n",
    "plt.plot(x_line, y_line, \"r\")\n",
    "plt.show()\n",
    "\n",
    "# Predict\n",
    "sess.run(H, feed_dict={X:100})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Data Loadaing\n",
    "data = pd.read_csv(\"./data/ozone/ozone.csv\")\n",
    "df = data.dropna(how=\"any\", inplace=False)\n",
    "\n",
    "# 독립변수와 종속변수를 뽑는다.\n",
    "x = df[\"Temp\"]\n",
    "y = df[\"Ozone\"]\n",
    "\n",
    "result = stats.linregress(x, y)\n",
    "# LinregressResult(slope=2.439109905529362, intercept=-147.64607238059494,rvalue=0.6985414096486389, pvalue=1.552677229392932e-17, stderr=0.23931937849409174)\n",
    "#                    slope = W (기울기)      intercept = b                  상관계수\n",
    "W = result[0]\n",
    "b = result[1]\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, W*x+b , \"r\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 1062.1995849609375\n",
      "cost : 5.628521919250488\n",
      "cost : 3.734180450439453\n",
      "cost : 2.6139659881591797\n",
      "cost : 1.9447864294052124\n",
      "cost : 1.5388127565383911\n",
      "cost : 1.286676049232483\n",
      "cost : 1.1247775554656982\n",
      "cost : 1.0161031484603882\n",
      "cost : 0.9390667676925659\n",
      "cost : 0.8810674548149109\n",
      "cost : 0.8347728848457336\n",
      "cost : 0.795906662940979\n",
      "cost : 0.7618837356567383\n",
      "cost : 0.7312542200088501\n",
      "cost : 0.7030786871910095\n",
      "cost : 0.6768483519554138\n",
      "cost : 0.6522125601768494\n",
      "cost : 0.6289304494857788\n",
      "cost : 0.6068653464317322\n",
      "cost : 0.5859397649765015\n",
      "cost : 0.5660207867622375\n",
      "cost : 0.5470702052116394\n",
      "cost : 0.5290359258651733\n",
      "cost : 0.5118738412857056\n",
      "cost : 0.49550652503967285\n",
      "cost : 0.4799317419528961\n",
      "cost : 0.46508893370628357\n",
      "cost : 0.45094871520996094\n",
      "cost : 0.43748512864112854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[151.9873]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple linear regression ( 행렬의 곱 형태  H = XW + b) X W가 행렬의 곱\n",
    "import tensorflow as tf\n",
    "\n",
    "# training data set\n",
    "x_data = [[73,80,75],\n",
    "          [93,88,93],\n",
    "          [89,91,90],\n",
    "          [96,98,100],\n",
    "          [73,66,70]]\n",
    "y_data = [[152],\n",
    "          [185],\n",
    "          [180],\n",
    "          [196],\n",
    "          [142]]\n",
    "\n",
    "# placeholder (2차원 matrix에서는 shape(placeholder가 받을 데이터 형태)를 반드시 지정해야 한다.)\n",
    "X = tf.placeholder(shape=[None,3], dtype = tf.float32)\n",
    "#                       None : 행의 개수는 고려하지 않겠다는 의미, 열은 무조건 3열\n",
    "Y = tf.placeholder(shape=[None,1], dtype = tf.float32)\n",
    "# 사용할 데이터 형태에 따라서 placeholder의 shape을 지정해야 한다!!\n",
    "# 아까 1차원에서는 따로 shape을 지정하지 않아도 되었다.\n",
    "\n",
    "# weight & bias\n",
    "W = tf.Variable(tf.random_normal([3,1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis (행렬 곱!!! matmul)\n",
    "# 기존 H = W*x + b\n",
    "H = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean( tf.square(H - Y))\n",
    "# 그 다음부터는 동일과정 진행\n",
    "optimizer = tf.train.GradientDescentOptimizer( learning_rate=0.00001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X:x_data, Y:y_data} )\n",
    "    if step % 1000 ==0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# Predict\n",
    "sess.run(H, feed_dict={X:[[73,80,75]]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\ipykernel\\__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\ipykernel\\__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\ipykernel\\__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 2.1787331104278564\n",
      "cost : 0.023908765986561775\n",
      "cost : 0.01677408628165722\n",
      "cost : 0.01570120081305504\n",
      "cost : 0.01553633064031601\n",
      "cost : 0.015510988421738148\n",
      "cost : 0.015507092699408531\n",
      "cost : 0.015506492927670479\n",
      "cost : 0.015506401658058167\n",
      "cost : 0.015506385825574398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[46.18512]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Loadaing\n",
    "data = pd.read_csv(\"./data/ozone/ozone.csv\")\n",
    "df = data.dropna(how=\"any\", inplace=False)\n",
    "\n",
    "# Nomalization \n",
    "df[\"Solar.R_Norm\"] = (df[\"Solar.R\"] - df[\"Solar.R\"].min()) / (df[\"Solar.R\"].max()-df[\"Solar.R\"].min())      \n",
    "df[\"Wind_Norm\"]= (df[\"Wind\"] - df[\"Wind\"].min()) / (df[\"Wind\"].max()-df[\"Wind\"].min())\n",
    "df[\"Temp_Norm\"] = (df[\"Temp\"] - df[\"Temp\"].min()) / (df[\"Temp\"].max()-df[\"Temp\"].min())      \n",
    "\n",
    "df[\"Ozone_Norm\"] = (df[\"Ozone\"]- df[\"Ozone\"].min()) / (df[\"Ozone\"].max()-df[\"Ozone\"].min())\n",
    "\n",
    "x_data = df[[\"Solar.R_Norm\",\"Wind_Norm\",\"Temp_Norm\"]]\n",
    "y_data = df[[\"Ozone_Norm\"]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 3], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# weight & bias\n",
    "W = tf.Variable(tf.random_normal([3,1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis (행렬 곱!!! matmul)\n",
    "# 기존 H = W*x + b\n",
    "H = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "# 그 다음부터는 동일과정 진행\n",
    "optimizer = tf.train.GradientDescentOptimizer( learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X:x_data, Y:y_data} )\n",
    "    if step % 300 ==0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "\n",
    "\n",
    "# Predict\n",
    "sess.run(H, feed_dict={X:[[190,7.4,67]]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55963303, 0.27717391, 0.25      ],\n",
       "       [0.33944954, 0.30978261, 0.375     ],\n",
       "       [0.43425076, 0.55978261, 0.425     ],\n",
       "       [0.93577982, 0.5       , 0.125     ],\n",
       "       [0.89296636, 0.3423913 , 0.2       ],\n",
       "       [0.28134557, 0.625     , 0.05      ],\n",
       "       [0.03669725, 0.9673913 , 0.1       ],\n",
       "       [0.76146789, 0.40217391, 0.3       ],\n",
       "       [0.86544343, 0.375     , 0.225     ],\n",
       "       [0.81651376, 0.4673913 , 0.275     ],\n",
       "       [0.17737003, 0.5923913 , 0.025     ],\n",
       "       [1.        , 0.5       , 0.175     ],\n",
       "       [0.91743119, 0.52717391, 0.225     ],\n",
       "       [0.21712538, 0.875     , 0.        ],\n",
       "       [0.96330275, 0.5       , 0.275     ],\n",
       "       [0.11314985, 0.40217391, 0.125     ],\n",
       "       [0.0030581 , 0.40217391, 0.05      ],\n",
       "       [0.95718654, 0.77717391, 0.4       ],\n",
       "       [0.05504587, 0.40217391, 0.1       ],\n",
       "       [0.25993884, 0.52717391, 0.1       ],\n",
       "       [0.01834862, 0.52717391, 0.25      ],\n",
       "       [0.74923547, 0.68478261, 0.6       ],\n",
       "       [0.66055046, 0.18478261, 0.55      ],\n",
       "       [0.83180428, 0.27717391, 0.475     ],\n",
       "       [0.36697248, 0.40217391, 0.625     ],\n",
       "       [0.86850153, 0.625     , 0.825     ],\n",
       "       [0.96636086, 0.5       , 0.75      ],\n",
       "       [0.43119266, 0.30978261, 0.625     ],\n",
       "       [0.56269113, 0.68478261, 0.5       ],\n",
       "       [0.8470948 , 1.        , 0.375     ],\n",
       "       [0.09174312, 0.375     , 0.2       ],\n",
       "       [0.34556575, 0.5       , 0.4       ],\n",
       "       [0.39755352, 0.43478261, 0.475     ],\n",
       "       [0.80122324, 0.09782609, 0.675     ],\n",
       "       [0.73700306, 0.375     , 0.7       ],\n",
       "       [0.70030581, 0.375     , 0.6       ],\n",
       "       [0.51376147, 0.125     , 0.65      ],\n",
       "       [0.93883792, 0.4673913 , 0.65      ],\n",
       "       [0.82262997, 0.15217391, 0.775     ],\n",
       "       [0.79510703, 0.2173913 , 0.875     ],\n",
       "       [0.81039755, 0.18478261, 0.875     ],\n",
       "       [0.51376147, 0.27717391, 0.8       ],\n",
       "       [0.78593272, 0.65217391, 0.4       ],\n",
       "       [0.51376147, 0.68478261, 0.6       ],\n",
       "       [0.12538226, 0.65217391, 0.575     ],\n",
       "       [0.77370031, 0.25      , 0.6       ],\n",
       "       [0.81651376, 0.43478261, 0.625     ],\n",
       "       [0.85015291, 0.2173913 , 0.675     ],\n",
       "       [0.55045872, 0.15217391, 0.75      ],\n",
       "       [0.65137615, 0.5       , 0.7       ],\n",
       "       [0.        , 0.25      , 0.425     ],\n",
       "       [0.87767584, 0.3423913 , 0.725     ],\n",
       "       [0.66055046, 0.30978261, 0.7       ],\n",
       "       [0.22629969, 0.3423913 , 0.625     ],\n",
       "       [0.2293578 , 0.52717391, 0.725     ],\n",
       "       [0.62996942, 0.27717391, 0.775     ],\n",
       "       [0.81957187, 0.27717391, 0.725     ],\n",
       "       [0.75229358, 0.27717391, 0.65      ],\n",
       "       [0.75535168, 0.375     , 0.6       ],\n",
       "       [0.2324159 , 0.25      , 0.6       ],\n",
       "       [0.05198777, 0.625     , 0.6       ],\n",
       "       [0.21406728, 0.27717391, 0.625     ],\n",
       "       [0.75840979, 0.0923913 , 0.8       ],\n",
       "       [0.67889908, 0.43478261, 0.825     ],\n",
       "       [0.6116208 , 0.30978261, 0.825     ],\n",
       "       [0.56574924, 0.5       , 0.725     ],\n",
       "       [0.81345566, 0.5       , 0.625     ],\n",
       "       [0.4587156 , 0.40217391, 0.575     ],\n",
       "       [0.19571865, 0.43478261, 0.5       ],\n",
       "       [0.13455657, 0.2173913 , 0.55      ],\n",
       "       [0.33027523, 0.27717391, 0.475     ],\n",
       "       [0.72477064, 0.4673913 , 0.525     ],\n",
       "       [0.55963303, 0.43478261, 0.525     ],\n",
       "       [0.7706422 , 0.7173913 , 0.5       ],\n",
       "       [0.08868502, 0.65217391, 0.375     ],\n",
       "       [0.62691131, 0.40217391, 0.55      ],\n",
       "       [0.70642202, 0.05978261, 0.6       ],\n",
       "       [0.63608563, 0.30978261, 0.725     ],\n",
       "       [0.59938838, 0.40217391, 1.        ],\n",
       "       [0.66666667, 0.        , 0.925     ],\n",
       "       [0.70336391, 0.2173913 , 0.975     ],\n",
       "       [0.55351682, 0.2173913 , 0.925     ],\n",
       "       [0.48929664, 0.25      , 0.85      ],\n",
       "       [0.58103976, 0.15217391, 0.875     ],\n",
       "       [0.5382263 , 0.02717391, 0.9       ],\n",
       "       [0.55657492, 0.125     , 0.9       ],\n",
       "       [0.26911315, 0.27717391, 0.75      ],\n",
       "       [0.25993884, 0.7173913 , 0.675     ],\n",
       "       [0.74923547, 0.4673913 , 0.575     ],\n",
       "       [0.65137615, 0.43478261, 0.525     ],\n",
       "       [0.68195719, 0.4673913 , 0.45      ],\n",
       "       [0.7706422 , 0.40217391, 0.4       ],\n",
       "       [0.70030581, 0.68478261, 0.6       ],\n",
       "       [0.7706422 , 0.7173913 , 0.475     ],\n",
       "       [0.70642202, 0.2173913 , 0.5       ],\n",
       "       [0.05198777, 0.4673913 , 0.35      ],\n",
       "       [0.32110092, 0.5       , 0.35      ],\n",
       "       [0.70336391, 0.25      , 0.525     ],\n",
       "       [0.66360856, 0.625     , 0.25      ],\n",
       "       [0.06116208, 0.43478261, 0.475     ],\n",
       "       [0.70642202, 0.43478261, 0.275     ],\n",
       "       [0.59327217, 0.30978261, 0.625     ],\n",
       "       [0.70642202, 0.55978261, 0.175     ],\n",
       "       [0.02140673, 0.375     , 0.35      ],\n",
       "       [0.40366972, 0.43478261, 0.6       ],\n",
       "       [0.12844037, 0.43478261, 0.3       ],\n",
       "       [0.03975535, 0.77717391, 0.15      ],\n",
       "       [0.56880734, 0.25      , 0.325     ],\n",
       "       [0.56269113, 0.65217391, 0.45      ],\n",
       "       [0.37920489, 0.30978261, 0.475     ],\n",
       "       [0.66055046, 0.5       , 0.275     ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data Loadaing\n",
    "data = pd.read_csv(\"./data/ozone/ozone.csv\", sep=\",\")\n",
    "df = data.dropna(how=\"any\", inplace=False)\n",
    " \n",
    "# 2. training data set  ( 데이터 정규화 !!!! )\n",
    "x_data = MinMaxScaler().fit_transform(df[[\"Solar.R\", \"Wind\",\"Temp\"]].values)\n",
    "y_data = MinMaxScaler().fit_transform(df[\"Ozone\"].values.reshape(-1,1))\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 27.052173614501953\n",
      "cost : 0.04766453430056572\n",
      "cost : 0.04365210235118866\n",
      "cost : 0.04354060813784599\n",
      "cost : 0.04353749752044678\n",
      "cost : 0.04353741928935051\n",
      "cost : 0.04353741183876991\n",
      "cost : 0.04353742301464081\n",
      "cost : 0.04353742301464081\n",
      "cost : 0.04353742301464081\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([5.973688], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Traning data set\n",
    "x_data = [1,2,5,8,10]\n",
    "y_data = [0,0,0,1,1]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(dtype = tf.float32)\n",
    "Y = tf.placeholder(dtype= tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "b= tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "H = W * X + b\n",
    "\n",
    "# Cost(Loss) function - 최소제곱법\n",
    "# Cost 함수의 값이 최소가 되는 W, b를 찾는 게 목적\n",
    "cost = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "# train node를 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val, = sess.run([train, cost], feed_dict = {X:x_data, Y:y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "plt.scatter(x_data, y_data)\n",
    "plt.plot(x_data, sess.run(W) * x_data + sess.run(b), \"r\" )\n",
    "plt.show()\n",
    "X = (0.5 - sess.run(b)) / sess.run(W)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "# % magic function : jupyter에서만 사용할 수 있는 기능\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn    # Sample Data를 가져오기 위한 utility module\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings   # warning 제어를 목적으로 사용\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")   #warning을 표시하지 않음\n",
    "\n",
    "# x : x parameter( 2개 )\n",
    "# y : lable( o or 1)\n",
    "x,y = mglearn.datasets.make_forge()\n",
    "\n",
    "mglearn.discrete_scatter(x[:,0], x[:,1], y)\n",
    "# y 가 1인 것 동그라미, 0인 것 세모로 그려짐!\n",
    "model = LogisticRegression()\n",
    "clf = model.fit(x, y)  # 학습\n",
    "\n",
    "mglearn.plots.plot_2d_separator(clf,x,fill=False, eps=0.5)\n",
    "# 그림 상에서 y 값을 0과 1로 구분하는 (영역을 나누는)선을 찾는 것이 Logistic regression \n",
    "# 입력 데이터의 영역을 구분해주는 것\n",
    "# 데이터가 어떤 영역에 들어가는지 알아보는 것\n",
    "# linear regression : \n",
    "#               데이터를 가장 잘 표현할 수 있는 직선을 그리고, 그 직선을 이용해서 예측하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow를 이용한 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 4.512444496154785\n",
      "cost : 0.42396727204322815\n",
      "cost : 0.3656659722328186\n",
      "cost : 0.32474732398986816\n",
      "cost : 0.2987504005432129\n",
      "cost : 0.2830456793308258\n",
      "cost : 0.2727740406990051\n",
      "cost : 0.26497671008110046\n",
      "cost : 0.2583981454372406\n",
      "cost : 0.25257208943367004\n",
      "정확도 : 0.8571428656578064\n",
      "예측 값 : [[1.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Data Loading\n",
    "# 파일이나 network를 통해서 데이터를 로딩한 후 전처리 과정\n",
    "\n",
    "# training data set( 2차원 matrix )\n",
    "x_data = [[10, 0],\n",
    "         [8, 1],\n",
    "         [3, 3],\n",
    "         [2, 3],\n",
    "         [5, 1],\n",
    "         [2,0],\n",
    "         [1,0]]\n",
    "\n",
    "y_data = [[1], [1], [1], [1], [0], [0], [0]] \n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias \n",
    "W = tf.Variable(tf.random_normal([2,1]), name = \"weight\")\n",
    "# X의 열 값과 W의 행 값이 같아야 행렬 곱을 할 수 있다.\n",
    "# Y의 열 값과 W의 열 값이 같아야 한다.\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X, W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost function\n",
    "# cost = tf.reduce_mean( y * tf.log(H) - (1-Y) * tf.log(1-H) )\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    if step % 300 ==0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# 우리가 만든 모델이 얼마나 정확한지를 측정\n",
    "# accuracy\n",
    "predict = tf.cast( H > 0.5, dtype=tf.float32 )\n",
    "# H가 0.5보다 크니? 결과가 True or False로 나온 것을 cast(True or False)\n",
    "#                                                     1.0      0.0\n",
    "correct = tf.equal(predict, Y)  # 예측 값과 Y를 비교해서 T, F로 나옴\n",
    "accuracy = tf.reduce_mean(tf.cast( correct, dtype = tf.float32 ))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})))\n",
    "# 정확도가 97% 이상 정도 되어야 사용할 수 있는 모델.\n",
    "\n",
    "# prediction\n",
    "print(\"예측 값 : {}\".format(sess.run( predict, feed_dict = {X:[[7, 1]]} )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 3.508707046508789\n",
      "cost : 0.34455522894859314\n",
      "cost : 0.2846040427684784\n",
      "cost : 0.27101853489875793\n",
      "cost : 0.26308706402778625\n",
      "cost : 0.2566627562046051\n",
      "cost : 0.25099509954452515\n",
      "cost : 0.24586546421051025\n",
      "cost : 0.24117109179496765\n",
      "cost : 0.2368445098400116\n",
      "정확도 : 0.8571428656578064\n",
      "예측값 : [[1.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[10, 0],\n",
    "         [8, 1],\n",
    "         [3, 3],\n",
    "         [2, 3],\n",
    "         [5, 1],\n",
    "         [2,0],\n",
    "         [1,0]]\n",
    "\n",
    "y_data = [[1], [1], [1], [1], [0], [0], [0]] \n",
    "\n",
    "# placeholder 생성\n",
    "X = tf.placeholder(shape = [None, 2], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 1], dtype = tf.float32)\n",
    "\n",
    "# weight & bias \n",
    "W = tf.Variable( tf.random_normal([2, 1]), name = \"weight\" )\n",
    "b = tf.Variable( tf.random_normal([1]), name = \"bias\" )\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X, W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# cost funcgion \n",
    "cost = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=Y))\n",
    "\n",
    "# train \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict = {X:x_data, Y:y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "\n",
    "# accuracy\n",
    "predict = tf.cast( H > 0.5, dtype = tf.float32)\n",
    "correct = tf.equal(predict, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})))\n",
    "print(\"예측값 : {}\".format(sess.run( predict, feed_dict={X:[[7, 1]]} )))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순서 기억하기\n",
    "- 1. data set 만들기 ( x_data, y_data )\n",
    "- 2. placeholder 생성 ( X, Y )\n",
    "- 3. Weight & bias 생성 \n",
    "- 4. Hypothesis 세우기\n",
    "- 5. cost function\n",
    "- 6. train\n",
    "- 7. Session 초기화\n",
    "- 8. 학습\n",
    "- 9. 예측모델 정확도 측정\n",
    "-    predict\n",
    "-    correct\n",
    "-    accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic 분석 ( Tensorflow )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 0.7274895310401917\n",
      "cost : 0.5069065690040588\n",
      "cost : 0.4765026271343231\n",
      "cost : 0.4689052104949951\n",
      "cost : 0.4660990834236145\n",
      "cost : 0.46464404463768005\n",
      "cost : 0.4636590778827667\n",
      "cost : 0.46287134289741516\n",
      "cost : 0.4621840715408325\n",
      "cost : 0.46156033873558044\n",
      "정확도 : 0.7833894491195679\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Data Loading\n",
    "data = pd.read_csv(\"./data/titanic/titanic_data.csv\")\n",
    "data_x = data[[\"Sex\", \"Age\", \"Pclass\", \"Fare\"]]\n",
    "data_y = data[\"Survived\"]\n",
    "data_x\n",
    "# 데이터가 숫자가 아니고, 숫자여도 의미가 수의 개념이 아닐 때 (의미가 명확하지 않을 때)\n",
    "# '전처리'가 필요하다!!\n",
    "\n",
    "# data 전처리\n",
    "Pclass_dummies = pd.get_dummies(data_x[\"Pclass\"], prefix = \"Pclass\")\n",
    "# Pclass의 1,2,3을 분리하기 위해 컬럼 세 개의 더미 컬럼으로 만들어서 사용하기\n",
    "data_x = data_x.join(Pclass_dummies)\n",
    "data_x.drop(\"Pclass\", axis=1, inplace=True) #Pclass를 더미 컬럼으로 분류하고 원본 컬럼 지우기\n",
    "Sex_dummies = pd.get_dummies(data_x[\"Sex\"], prefix = \"Sex\")\n",
    "data_x = data_x.join(Sex_dummies)\n",
    "data_x.drop(\"Sex\", axis=1, inplace=True)\n",
    "\n",
    "# training data set\n",
    "x_data = MinMaxScaler().fit_transform(data_x.values)\n",
    "# MinMaxScaler().fit_transform() : 이상치 데이터 조절하기 위해 데이터 정규화\n",
    "# data_x.values : DataFrame을 2차원 numpy array로 가져오기\n",
    "y_data = data_y.values.reshape(-1,1)   \n",
    "# 1차원을 2차원으로 바꾸기( 행 수는 알 수 없고 열은 1개)\n",
    "# x_data.shape  ( 891,7)\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 7], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 1], dtype = tf.float32)\n",
    "\n",
    "# weight & bias\n",
    "W = tf.Variable(tf.random_normal([7,1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X, W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=Y))\n",
    "\n",
    "# train \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(10000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 1000 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# accuracy 측정\n",
    "predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "correct = tf.equal(predict, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admission분석\n",
    "#### 주어진 데이터의 70%를 training용으로 사용\n",
    "#### 나머지 30%를 test용으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:353: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:354: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 1.4096524715423584\n",
      "cost : 0.6818090677261353\n",
      "cost : 0.6209319829940796\n",
      "cost : 0.58918696641922\n",
      "cost : 0.5736782550811768\n",
      "cost : 0.5658023953437805\n",
      "cost : 0.5613945126533508\n",
      "cost : 0.5586307048797607\n",
      "cost : 0.5567122101783752\n",
      "cost : 0.5552744269371033\n",
      "--------------------------------------------------\n",
      "나머지 30% 데이터로 테스트한 정확도(%) : 67.5000011920929\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data Loading\n",
    "data = pd.read_csv(\"./data/admission/admission.csv\")\n",
    "\n",
    "# data70p = data[:280]\n",
    "# data30p = data.iloc[280:400]\n",
    "data70p = data[ 0 : int(data.shape[0]*0.7) ]\n",
    "# 상위 70%를 training data로 사용할 것이기 떄문에,\n",
    "# data.shape[0] : 전체 데이터 row 수의 0.7 개\n",
    "data30p = data[ int(data.shape[0]*0.7) : ]\n",
    "\n",
    "data_x_train = data70p[[\"gre\", \"gpa\", \"rank\"]]\n",
    "data_y_train = data70p[\"admit\"]\n",
    "data_x_test = data30p[[\"gre\", \"gpa\", \"rank\"]]\n",
    "data_y_test = data30p[\"admit\"]\n",
    "\n",
    "# Data 전처리 과정\n",
    "rank_dummies = pd.get_dummies(data_x_train[\"rank\"], prefix=\"rank\")\n",
    "data_x_train = data_x_train.join(rank_dummies)\n",
    "data_x_train.drop(\"rank\", axis=1, inplace=True)\n",
    "\n",
    "data_x_test = data_x_test.join(rank_dummies)\n",
    "data_x_test.drop(\"rank\", axis=1, inplace=True)\n",
    "\n",
    "# training data set\n",
    "x_data_train = MinMaxScaler().fit_transform(data_x_train.values)\n",
    "x_data_test = MinMaxScaler().fit_transform(data_x_test.values)\n",
    "\n",
    "y_data_train = data_y_train.values.reshape(-1,1) \n",
    "y_data_test = data_y_test.values.reshape(-1,1) \n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape = [None, 6], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 1], dtype = tf.float32)\n",
    "\n",
    "# weight & bias\n",
    "W = tf.Variable(tf.random_normal([6,1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(X, W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(10000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X:x_data_train, Y:y_data_train})\n",
    "    if step % 1000 ==0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# accuracy\n",
    "predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "correct = tf.equal(predict, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"-\"* 50)\n",
    "print(\"나머지 30% 데이터로 테스트한 정확도(%) : {}\"\\\n",
    "      .format( 100 * sess.run(accuracy, feed_dict={X:x_data_test, Y:y_data_test})))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.573147\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0.172627\n",
       "1      0.292175\n",
       "2      0.738408\n",
       "3      0.178385\n",
       "4      0.118354\n",
       "5      0.369970\n",
       "6      0.419246\n",
       "7      0.217003\n",
       "8      0.200735\n",
       "9      0.517868\n",
       "10     0.374314\n",
       "11     0.400200\n",
       "12     0.720539\n",
       "13     0.353455\n",
       "14     0.692380\n",
       "15     0.185825\n",
       "16     0.339939\n",
       "17     0.078953\n",
       "18     0.540228\n",
       "19     0.573512\n",
       "20     0.161221\n",
       "21     0.437271\n",
       "22     0.128375\n",
       "23     0.192049\n",
       "24     0.437594\n",
       "25     0.682295\n",
       "26     0.578481\n",
       "27     0.204754\n",
       "28     0.423073\n",
       "29     0.458299\n",
       "         ...   \n",
       "370    0.398574\n",
       "371    0.317087\n",
       "372    0.376508\n",
       "373    0.530854\n",
       "374    0.411424\n",
       "375    0.187357\n",
       "376    0.415124\n",
       "377    0.589590\n",
       "378    0.202240\n",
       "379    0.218961\n",
       "380    0.463667\n",
       "381    0.346029\n",
       "382    0.349677\n",
       "383    0.672759\n",
       "384    0.186651\n",
       "385    0.351893\n",
       "386    0.528429\n",
       "387    0.342879\n",
       "388    0.339081\n",
       "389    0.402750\n",
       "390    0.400936\n",
       "391    0.487194\n",
       "392    0.222029\n",
       "393    0.438725\n",
       "394    0.253423\n",
       "395    0.488670\n",
       "396    0.165504\n",
       "397    0.181062\n",
       "398    0.463667\n",
       "399    0.300731\n",
       "Length: 400, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data Loading\n",
    "data = pd.read_csv(\"./data/admission/admission.csv\")\n",
    "data_x = data.drop(\"admit\", axis=1)\n",
    "data_y = data[\"admit\"]\n",
    "rank_dummies = pd.get_dummies(data_x[\"rank\"], prefix=\"rank\")\n",
    "\n",
    "data_x = data_x.join(rank_dummies)\n",
    "data_x.drop(\"rank\", axis=1, inplace=True)\n",
    "logit = sm.Logit(data_y, data_x)   # 모델생성\n",
    "result = logit.fit()     # 학습과정\n",
    "result.predict(data_x)   #예측\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning 개념 정리!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Supervised Learning (지도학습)\n",
    "  - training data set에 label이 붙어 있다.!!\n",
    "\n",
    "     => 1.Simple Linear Regression\n",
    "        독립변수(입력 parameter)가 1개인 것을 지칭\n",
    "        Hypothesis => H = W * X + b, Cost function\n",
    "     => 2.Multiple Linear Regression\n",
    "        독립변수(입력 prarameter)가 2개 이상인 것 지칭\n",
    "        Hypothesis => H = XW + b\n",
    "     => 3.Logistic Regression (label이 0 or 1)\n",
    "        실제로 현실에서 많이 이용되고 있는 학습 모델\n",
    "        정확도를 측정가능.\n",
    "        다른말로 binary classification\n",
    "        Hypothesis => sigmoid(XW + b)\n",
    "\n",
    "### - Unsupervised Learning (비지도학습)\n",
    "   - training data set에 label이 붙어 있지 않은 경우!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Regression : sigmoid를 쓰는 대신에 softmax를 사용\n",
    "기존에는 Y label이 하나였는데, Y label이 n개로 늘어남\n",
    "\n",
    "one-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 13.154136657714844\n",
      "cost : 0.6375163197517395\n",
      "cost : 0.5527145266532898\n",
      "cost : 0.5127300024032593\n",
      "cost : 0.4837382435798645\n",
      "cost : 0.4598611295223236\n",
      "cost : 0.4391050338745117\n",
      "cost : 0.4205504357814789\n",
      "cost : 0.40368932485580444\n",
      "cost : 0.3882047235965729\n",
      "정확도 : 0.8571428656578064\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. data loading\n",
    "# 2. training data set\n",
    "x_data = [[10,7,8,5],\n",
    "         [8,8,9,4],\n",
    "         [7,8,2,3],\n",
    "         [6,3,9,3],\n",
    "         [7,5,7,4],\n",
    "         [3,5,6,2],\n",
    "         [2,4,3,1]]\n",
    "\n",
    "y_data = [[1,0,0], \n",
    "         [1,0,0],\n",
    "         [0,1,0],\n",
    "         [0,1,0],\n",
    "         [0,1,0],\n",
    "         [0,0,1],\n",
    "         [0,0,1]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 4] , dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 3] , dtype=tf.float32)\n",
    "\n",
    "# weight & bias\n",
    "W = tf.Variable(tf.random_normal([4,3]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([3]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X, W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# Cost(Loss) function\n",
    "cost = \\\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)   \n",
    "# argmax : argument로 들어오는 것 중에 가장 큰 것이 몇 번째에 있는지 인덱스 알려줌\n",
    "# ex) H => [[0.3   0.6   0.1]]이면 가장 큰 것이 0.6인 B이고, \n",
    "# 1은 축 방향 중 '열'에서 찾아서 인덱스 번호를 알려달라는 뜻이다. 결과는 인덱스 번호 1이 나옴!\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))   \n",
    "#correct 계산해서 평균내는 작업, cast : True =1, False =0으로 바꿔주는 작업\n",
    "# 우리가 세운 가설 H를 통해 가장 큰 값이 어디 있는지 인덱스로 알려준다. \n",
    "# => 이것은 thin, normal, fat 중 하나를 의미하는데\n",
    "# 이것과 실제 데이터인 Y에서 argmax를 통해 뽑아낸 인덱스와 비교를 하고,\n",
    "# 두 개가 같으면 True, 다르면 False를 반환하고,\n",
    "# tf.cast를 통해 True를 1로 전환하여 reduce_mean을 통해 평균을 계산해준다.\n",
    "# 이것은 우리가 세운 가설과 실제가 얼마나 들어 맞는지를 확인하기 위한 작업이다. (정확도)\n",
    "# 정확도를 97% 이상으로 올려야 유의미한 데이터로 사용할 수 있다.\n",
    "# 높은 정확도를 갖는 모델을 통해 예측을 하면 유의미하게 활용할 수 있다.\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})))\n",
    "\n",
    "# 예측\n",
    "result = sess.run(predict, feed_dict={X:[[5,3,9,4]]})\n",
    "# array([0], dtype=int64) => A grade\n",
    "if result[0]==0:\n",
    "    print(\"A\")\n",
    "elif result[0]==1:\n",
    "    print(\"B\")\n",
    "else:\n",
    "    print(\"C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원 핫 인코딩(One-hot encoding) \n",
    "#### : 단어 집합의 크기를 벡터의 크기로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식\n",
    "\n",
    "(1) 각 단어에 고유한 인덱스를 부여합니다.\n",
    "\n",
    "(2) 표현하고 싶은 단어의 인덱스의 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여합니다.\n",
    "\n",
    "### MinMaxScaler.fit_transform\n",
    "#### : 최소값(Min)과 최대값(Max)을 사용해서 '0~1' 사이의 범위(range)로 데이터를 표준화해주는 '0~1 변환'\n",
    "\n",
    "Scaler 클래스의 사용 방법\n",
    "\n",
    "(1) 클래스 객체 생성\n",
    "\n",
    "(2) fit() 메서드와 트레이닝 데이터를 사용하여 변환 계수 추정\n",
    "\n",
    "(3) transform() 메서드를 사용하여 실제로 자료를 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BMI 예제 (multinomial classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 1.3295873403549194\n",
      "cost : 0.497956246137619\n",
      "cost : 0.40565887093544006\n",
      "cost : 0.35655251145362854\n",
      "cost : 0.3241330683231354\n",
      "cost : 0.3004722595214844\n",
      "cost : 0.28213730454444885\n",
      "cost : 0.26734501123428345\n",
      "cost : 0.2550593316555023\n",
      "cost : 0.24462871253490448\n",
      "정확도 : 0.9786666631698608\n",
      "예측값 : [[0.1466186  0.7698086  0.08357278]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "# warning을 출력하지 않도록 지정\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# data loading\n",
    "data = pd.read_csv(\"./data/bmi/bmi.csv\", sep=\",\", skiprows=3)\n",
    "data = data.dropna(how=\"any\") # 결치값 제거\n",
    "num_of_data = int(data.shape[0] * 0.7)  # 데이터의 70%를 train data로 30%를 test로\n",
    "data_train = data.loc[ : num_of_data, : ]\n",
    "data_test = data.loc[num_of_data : , : ]\n",
    "\n",
    "df_train_x = data_train[[\"height\", \"weight\"]]\n",
    "df_train_y = data_train[\"label\"]\n",
    "df_test_x = data_test[[\"height\", \"weight\"]]\n",
    "df_test_y = data_test[\"label\"]\n",
    "\n",
    "# df_train_y => 0,1,2   0(thin)   => [1 0 0]\n",
    "#                 1(normal) => [0 1 0]\n",
    "#                 2(fat)    => [0 0 1]\n",
    "\n",
    "### one-hot encoding\n",
    "sess = tf.Session()\n",
    "scaler = MinMaxScaler()\n",
    "train_x_data = scaler.fit_transform(df_train_x.values)\n",
    "train_y_data = tf.one_hot(df_train_y, 3).eval(session=sess)\n",
    "# train_y_data = sess.run(tf.one_hot(df_train_y,3))이랑 같음\n",
    "test_x_data = scaler.transform(df_test_x.values)\n",
    "test_y_data = tf.one_hot(df_test_y, 3).eval(session=sess)\n",
    "\n",
    "#######################\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 3], dtype=tf.float32)\n",
    "\n",
    "# weight & bias\n",
    "W = tf.Variable(tf.random_normal([2,3]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([3]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X, W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# Cost\n",
    "cost = \\\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(10000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X:train_x_data, Y:train_y_data})\n",
    "    if step % 1000 ==0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "        \n",
    "# accuracy\n",
    "predict = tf.argmax(H, 1)   # H값 중에 최대 값의 인덱스를 알고 싶을 때 사용\n",
    "correct = tf.equal(predict, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, feed_dict={X:test_x_data, Y:test_y_data})))\n",
    "\n",
    "print(\"예측값 : {}\".format(sess.run(H, feed_dict={X:scaler.transform([[168,60]])})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST - Multinomial Classification\n",
    "입력데이터 - 이미지에 대한 pixel data가 들어온다.!\n",
    "\n",
    "원래 이미지 데이터는 3차원 데이터인데 \n",
    "\n",
    "흑백이고 2차원 데이터를 1차원으로 변환해서 입력을 받는다.\n",
    "\n",
    "약 5만 5천개의 이미지를 입력으로 받는다.\n",
    "\n",
    "입력데이터(x parameter)의 shape => (55000, 784)\n",
    "\n",
    "y축 label의 shape => (55000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "cost : 0.5336349010467529\n",
      "cost : 0.39516597986221313\n",
      "cost : 0.35359644889831543\n",
      "cost : 0.21989548206329346\n",
      "cost : 0.1977711170911789\n",
      "cost : 0.2932611405849457\n",
      "cost : 0.1949208825826645\n",
      "cost : 0.46046680212020874\n",
      "cost : 0.30638399720191956\n",
      "cost : 0.18828792870044708\n",
      "정확도 : 0.9164999723434448\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "    \n",
    "# 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "# one_hot=True : 데이터 읽어올 때 기본적으로 one_hot으로 만들어져서 가져옴\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.argmax(mnist.train.labels[0].reshape(-1,10),1))\n",
    "# # argmax(2차배열)\n",
    "# # array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]) => labels[0] 데이터가 '7'\n",
    "# plt.imshow(mnist.train.images[3].reshape(28,28), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "\n",
    "# 2. placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "#                              28*28\n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# 3. weight & bias\n",
    "W = tf.Variable(tf.random_normal([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([10]), name = \"bias\")\n",
    "\n",
    "# 4. Hypothesis\n",
    "logit =tf.matmul(X, W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# 5. cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "\n",
    "# 6.train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(cost)\n",
    "\n",
    "# 7.session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 8.학습 (epoch, batch처리 공부)\n",
    "train_epoch = 30\n",
    "batch_size = 100  # 반복처리 중 한 번에 읽어들일 데이터의 크기\n",
    "\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "    # 반복 횟수 ( 한 번에 100개씩 읽고, 전체 데이터 사이즈를 나누면 반복 횟수)\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X:batch_x, Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# accuracy\n",
    "predict = tf.argmax(H, 1)\n",
    "correct = tf.equal(predict, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "# 정확도 출력\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy,\\\n",
    "                        feed_dict={X:mnist.test.images, Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:asus_env]",
   "language": "python",
   "name": "conda-env-asus_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
