{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple layer을 이용한 XOR문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 12.10586166381836\n",
      "cost : 0.00040691159665584564\n",
      "cost : 0.0002600135048851371\n",
      "cost : 0.0001929043501149863\n",
      "cost : 0.0001538990472909063\n",
      "cost : 0.00012830118066631258\n",
      "cost : 0.00011017949873348698\n",
      "cost : 9.664895333116874e-05\n",
      "cost : 8.615187834948301e-05\n",
      "cost : 7.776331767672673e-05\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. training data set\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "# 2. placeholder\n",
    "X = tf.placeholder(shape = [None,2], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,1], dtype = tf.float32)\n",
    "\n",
    "# 3. weight & bias                    뒤에 2: logistic 2개 쓴다는 의미\n",
    "# W1 = tf.Variable(tf.random_normal([2,2]), name = \"weight1\")\n",
    "# b1 = tf.Variable(tf.random_normal([2]), name = \"bias1\")\n",
    "W1 = tf.Variable(tf.random_normal([2,256]), name = \"weight1\")\n",
    "#                                    20 :logistic 20개 뭉쳐서 사용하자!\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,512]), name = \"weight2\")\n",
    "#                                    20 :logistic 20개 뭉쳐서 사용하자!\n",
    "b2 = tf.Variable(tf.random_normal([512]), name = \"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([2,1]), name = \"weight2\")\n",
    "# b2 = tf.Variable(tf.random_normal([1]), name = \"bias2\")\n",
    "W3 = tf.Variable(tf.random_normal([512,1]), name = \"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([1]), name = \"bias3\")\n",
    "# logistic을 2개에서 20로 wide하게 넓히기. 정확도 높아짐\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# 4. cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(10000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 1000 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# accuracy\n",
    "predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "correct = tf.equal(predict, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, feed_dict = {X:x_data, Y:y_data})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST - Multinomial Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "cost : 1.2107499837875366\n",
      "cost : 0.47547444701194763\n",
      "cost : 0.18839482963085175\n",
      "cost : 0.2259468138217926\n",
      "cost : 0.34110504388809204\n",
      "cost : 0.27594712376594543\n",
      "cost : 0.13481181859970093\n",
      "cost : 0.14410749077796936\n",
      "cost : 0.08090093731880188\n",
      "cost : 0.02610633336007595\n",
      "정확도 : 0.9189000129699707\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "### 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "### 2. placeholder\n",
    "# 입력데이터는 이미지 데이터이고, 원래 이미지 데이터는 3차원인데\n",
    "# MNIST는 흑백이기 때문에 2차원 형태의 이미지 데이터이고, \n",
    "# 처리를 쉽게 하기 위해 이미지 자체의 데이터를 1차원으로 표현\n",
    "# 28x28 이미지인데 1차원으로 표현 784의 열\n",
    "X = tf.placeholder(shape=[None, 784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 10], dtype = tf.float32)\n",
    "#                              10: 0~9까지 one_hot encoding으로 인해\n",
    "\n",
    "\n",
    "\n",
    "### 3. weight & bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name = \"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name = \"bias\")\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256]), name = \"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,512]), name = \"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([512]), name = \"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([512,10]), name = \"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "### 4. hypothesis\n",
    "logit = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.softmax(logit)  # 확률값으로 결과를 얻기 위해\n",
    "\n",
    "### 5. cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "\n",
    "### 6. train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "### 7. Session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "### 8. 학습\n",
    "train_epoch = 30\n",
    "batch_size = 100  # 반복처리 중 한 번에 읽어들일 데이터의 크기\n",
    "\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "    # 반복 횟수 ( 한 번에 100개씩 읽고, 전체 데이터 사이즈를 나누면 반복 횟수)\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X:batch_x, Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# accuracy\n",
    "predict = tf.argmax(H, 1)\n",
    "correct = tf.equal(predict, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "# 정확도 출력\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy,\\\n",
    "                        feed_dict={X:mnist.test.images, Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "cost : 0.35351866483688354\n",
      "cost : 0.2128538191318512\n",
      "cost : 0.10645133256912231\n",
      "cost : 0.06949642300605774\n",
      "cost : 0.10989005863666534\n",
      "cost : 0.054328642785549164\n",
      "cost : 0.026060037314891815\n",
      "cost : 0.06211819127202034\n",
      "cost : 0.030809974297881126\n",
      "cost : 0.015489592216908932\n",
      "정확도 : 0.9818999767303467\n"
     ]
    }
   ],
   "source": [
    "### MNIST - Neural Network (wide & deep)\n",
    "###        => Sigmoid => nn.relu로 변경 (ReLu :Rectified Linear Unit)\n",
    "###        => Xavier initialization을 도입해서 초기 W 값 지정\n",
    "###        => overfitting을 피하기 위해 dropout\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "### 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "### 2. placeholder\n",
    "# 입력데이터는 이미지 데이터이고, 원래 이미지 데이터는 3차원인데\n",
    "# MNIST는 흑백이기 때문에 2차원 형태의 이미지 데이터이고, \n",
    "# 처리를 쉽게 하기 위해 이미지 자체의 데이터를 1차원으로 표현\n",
    "# 28x28 이미지인데 1차원으로 표현 784의 열\n",
    "X = tf.placeholder(shape=[None, 784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 10], dtype = tf.float32)\n",
    "#                              10: 0~9까지 one_hot encoding으로 인해\n",
    "\n",
    "### 3. weight & bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name = \"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name = \"bias\")\n",
    "\n",
    "keep = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "# W1 = tf.Variable(tf.random_normal([784,256]), name = \"weight1\")\n",
    "W1 = tf.get_variable(\"weight1\", shape =[784,256], \\\n",
    "                           initializer=tf.contrib.layers.xavier_initializer() )\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob=keep)\n",
    "\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([256,512]), name = \"weight2\")\n",
    "W2 = tf.get_variable(\"weight2\", shape =[256,512], \\\n",
    "                           initializer=tf.contrib.layers.xavier_initializer() )\n",
    "\n",
    "b2 = tf.Variable(tf.random_normal([512]), name = \"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "layer1 = tf.nn.dropout(_layer2, keep_prob=keep)\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([512,10]), name = \"weight3\")\n",
    "W3 = tf.get_variable(\"weight3\", shape =[512,10], \\\n",
    "                           initializer=tf.contrib.layers.xavier_initializer() )\n",
    "\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "### 4. hypothesis\n",
    "H = tf.matmul(_layer2,W3) + b3\n",
    "# H = tf.nn.softmax(logit)  # 확률값으로 결과를 얻기 위해\n",
    "\n",
    "### 5. cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=H, labels=Y))\n",
    "\n",
    "### 6. train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "### 7. Session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "### 8. 학습\n",
    "train_epoch = 30\n",
    "batch_size = 100  # 반복처리 중 한 번에 읽어들일 데이터의 크기\n",
    "\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "    # 반복 횟수 ( 한 번에 100개씩 읽고, 전체 데이터 사이즈를 나누면 반복 횟수)\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X:batch_x, Y:batch_y, keep:0.7})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# accuracy\n",
    "predict = tf.argmax(H, 1)\n",
    "correct = tf.equal(predict, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "# 정확도 출력\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy,\\\n",
    "                        feed_dict={X:mnist.test.images, Y:mnist.test.labels, keep:1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-6389289e77e0>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\msm03\\Anaconda3\\envs\\asus_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "cost : 0.14306385815143585\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 1. Data Loading                               one_hot=True y label one hot형태 자동변환\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# 2. placeholder\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
    "\n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "keep_rate = tf.placeholder(dtype = tf.float32) #dropout 비율 (shape필요없음) \n",
    "# 입력parameter dropout 하기 위해 placeholder 잡아주기\n",
    "# overfitting되지 않도록 하기 위해 keep_rate 지정\n",
    "\n",
    "# 3. Convolution Layer\n",
    "# 3.1 Convolution Layer 1 (conv, relu, pool)\n",
    "#     입력데이터의 형태를 Convolution 할 수 있도록 4차배열로 reshape (None, 784) => \n",
    "X_img = tf.reshape(X, shape=[ -1 ,28 ,28, 1]) # -1은 남은 데이터를 이 형태로 만들어주라는 의미\n",
    "# 55000개를 28*28인 784로 만들고 나머지 데이터들도 다 이렇게 만들라는 의미???\n",
    "# 정해진 것 없음?\n",
    "\n",
    "# Filter를 생성\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))\n",
    "# 3*3의 흑백 필터 32개 / 표준편차를 적게 줘서 난수 값의 차이가 많이 나지 않도록 설정\n",
    "\n",
    "# Convolution\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1,1,1,1], padding=\"SAME\")\n",
    "# print(L1.shape): (?, 28, 28, 32)\n",
    "# ReLU\n",
    "L1 = tf.nn.relu(L1)\n",
    "# max pooling\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "L1.shape\n",
    "\n",
    "# 3.2 Convolution Layer 2   (위에 L1을 만드는 것과 같은 건데 한 줄에 합쳐서 만든 것.)\n",
    "#     Filter, Convolution, ReLU  \n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64,\\\n",
    "                kernel_size=[3,3], padding=\"SAME\", strides=1, activation=tf.nn.relu)\n",
    "# Max Pooling\n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2], padding=\"SAME\", strides=2)\n",
    "\n",
    "\n",
    "#### Convolution Layer 끝\n",
    "\n",
    "# 4. FC ( Neural Network )\n",
    "L2 = tf.reshape(L2, shape=[-1,7*7*64])\n",
    "\n",
    "\n",
    "# 5. Weight & bias                             결과를 256개 뽑겠다. output내가 지정\n",
    "W2 = tf.get_variable(\"weight2\", shape=[7*7*64, 256],\\\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name =\"bias2\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(L2, W2) + b2)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob=keep_rate)\n",
    " ############layer 늘리기 !!! #################\n",
    "W3 = tf.get_variable(\"weight3\", shape=[256, 256],\\\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]), name =\"bias3\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1, W3) + b3)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob=keep_rate)\n",
    "\n",
    "W4 = tf.get_variable(\"weight4\", shape=[256, 10],\\\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]), name =\"bias4\")\n",
    "\n",
    "# Hypothesis\n",
    "H = tf.matmul(layer2, W4) + b4\n",
    "\n",
    "# Cost funtcion\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=H, labels=Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 (batch 처리(조금씩 잘라서!) 하기!!! 데이터 사이즈가 너무 크고, 오래 걸림!!)\n",
    "num_of_epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], \\\n",
    "                               feed_dict={X:batch_x, Y:batch_y, keep_rate:0.5})\n",
    "    \n",
    "    print(\"cost : {}\".format(cost_val))\n",
    "\n",
    "# accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "result_sum = 0\n",
    "### accuracy 측정을 위해서 batch 처리\n",
    "num_of_iter = int(mnist.test.num_examples / batch_size)\n",
    "    \n",
    "for i in range(num_of_iter):\n",
    "    batch_x, batch_y = mnist.test.next_batch(batch_size)\n",
    "    correct_num = sess.run(accuracy, \\\n",
    "                               feed_dict={X:batch_x, Y:batch_y, keep_rate:1})\n",
    "        \n",
    "    result_sum += correct_num\n",
    "\n",
    "print(\"정확도 : {}\".format(result_sum / 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python 객체지향 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "60.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Student:   # Student라는 하나의 객체를 만드는 것\n",
    "    \n",
    "    def __init__(self,name,kor,eng,math):\n",
    "        self.name = name   #self.name : init이라는 생성자가 가지는 변수 (계속 남이있음)\n",
    "        self.kor = kor\n",
    "        self.eng = eng\n",
    "        self.math = math\n",
    "        \n",
    "    def calc_avg(self):\n",
    "        return (self.kor + self.eng + self.math) / 3\n",
    "    \n",
    "stu1 = Student(\"홍길동\", 10, 20 ,30)\n",
    "print(stu1.calc_avg())\n",
    "stu2 = Student(\"김길동\", 30, 60 ,90)\n",
    "print(stu2.calc_avg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개의 모델을 만들어서 관리를 해야 된다!\n",
    "# 각각의 모델을 class의 instance로 만들어서 관리해보자!\n",
    "# 우리가 만들 model의 데이터와 기능을 생각해서 class를 design을 해야한다!\n",
    "# class로부터 파생되는 인스턴스가 어떤 기능을 가지고 있을지 고민/생각하며 디자인 하기!! \n",
    "# 객체지향의 설계에는 정답이 없다. 잘 된 설계 / 구린 설계로 나뉠 뿐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CNNModel:\n",
    "    # 사용하는 데이터를 field(변수로 선언)\n",
    "    # 사용하는 기능은 함수(method)로 표현\n",
    "    # 데이터 로딩 없음!!!!!!!!!!!!!!\n",
    "    # - tensorflow graph를 그리는 기눙(모델을 구축)\n",
    "    # - 학습하는 기능\n",
    "    # - 정확도 측정\n",
    "    # - 예측작업(prediction)\n",
    "    # 우리가 사용할 데이터는 클래스 안에 넣기 보다는 \n",
    "    # 나중에 사용할 때 로딩하여 객체한테 넘겨줘서 사용하는 것이 일반적이다.\n",
    "    # 함수 만들 때, self를 붙여서 선언을 해주어야 밑에서 사용할 수 있다.\n",
    "     \n",
    "        # 무조건 생성자 정의해야 함!!! / class에서 인스턴스를 파생    \n",
    "    def __init__(self,name, sess): \n",
    "        self.name = name\n",
    "        self.sess = sess\n",
    "        \n",
    "    def build_model(self):\n",
    "        # placeholder부터 코드 작성\n",
    "        self.train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "        \n",
    "    def exec_train(self, x_data, y_data):\n",
    "        sess.run([train, cost], feed_dict={X:x_data , Y:y_data})\n",
    "        \n",
    "## 1. 데이터 로딩\n",
    "##    pandas를 이용해서 데이터를 읽는다\n",
    "## 2. 모델객체를 생성(10개)\n",
    "\n",
    "sess = tf.Session()\n",
    "model1 = CNNModel(\"model1\", sess)\n",
    "model2 = CNNModel(\"model2\", sess)\n",
    "model3 = CNNModel(\"model3\", sess)\n",
    "model1.build_model()  # model1의 gragh를 생성\n",
    "model1.exec_train()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:asus_env]",
   "language": "python",
   "name": "conda-env-asus_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
